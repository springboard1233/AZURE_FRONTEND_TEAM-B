{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e800aff-4af6-4ee1-82a8-3ac2e0bcbf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\AZURE_FRONTEND_TEAM-B\\.ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1a5266",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\susha\\\\OneDrive\\\\Desktop\\\\Azure Based Demand Forecasting & Capacity Optimization System\\\\csv file\\\\azure_usage.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Use the full path to your CSV files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m azure_data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43msusha\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mOneDrive\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDesktop\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mAzure Based Demand Forecasting & Capacity Optimization System\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcsv file\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mazure_usage.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m external_data = pd.read_csv(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33msusha\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mOneDrive\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mAzure Based Demand Forecasting & Capacity Optimization System\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mcsv file\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mexternal_factors.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Show first few rows to check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\susha\\\\OneDrive\\\\Desktop\\\\Azure Based Demand Forecasting & Capacity Optimization System\\\\csv file\\\\azure_usage.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the full path to your CSV files\n",
    "azure_data = pd.read_csv(r\"C:\\Users\\susha\\OneDrive\\Desktop\\Azure Based Demand Forecasting & Capacity Optimization System\\csv file\\azure_usage.csv\")\n",
    "external_data = pd.read_csv(r\"C:\\Users\\susha\\OneDrive\\Desktop\\Azure Based Demand Forecasting & Capacity Optimization System\\csv file\\external_factors.csv\")\n",
    "\n",
    "# Show first few rows to check\n",
    "print(azure_data.head())\n",
    "print(external_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0724e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azure_usage.csv', 'external_factors.csv.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r\"C:\\Users\\susha\\OneDrive\\Desktop\\Azure Based Demand Forecasting & Capacity Optimization System\\csv file\"\n",
    "print(os.listdir(folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb08f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date   region resource_type  usage_cpu  usage_storage  users_active\n",
      "0  2023-01-01  East US            VM         88           1959           470\n",
      "1  2023-01-01  East US       Storage         92           1595           388\n",
      "2  2023-01-01  East US     Container         70            621           414\n",
      "3  2023-01-01  West US            VM         60           1982           287\n",
      "4  2023-01-01  West US       Storage         85           1371           351\n",
      "         date  economic_index  cloud_market_demand  holiday\n",
      "0  2023-01-01          104.97                 0.99        1\n",
      "1  2023-01-02          106.48                 1.15        0\n",
      "2  2023-01-03           97.66                 0.98        0\n",
      "3  2023-01-04          115.79                 1.08        0\n",
      "4  2023-01-05           95.31                 1.05        0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = r\"C:\\Users\\susha\\OneDrive\\Desktop\\Azure Based Demand Forecasting & Capacity Optimization System\\csv file\"\n",
    "\n",
    "azure_data = pd.read_csv(os.path.join(folder_path, \"azure_usage.csv\"))\n",
    "external_data = pd.read_csv(os.path.join(folder_path, \"external_factors.csv.csv\"))\n",
    "\n",
    "print(azure_data.head())\n",
    "print(external_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find data files in 'data/raw'. Please check the file paths.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- MILESTONE 2: FEATURE ENGINEERING ---\n",
    "\n",
    "# --- 1. Load the Merged Data ---\n",
    "# Ensure your CSV files are in the 'data/raw' folder\n",
    "usage_path = 'data/raw/azure_usage.csv'\n",
    "factors_path = 'data/raw/external_factors.csv.csv'\n",
    "processed_folder = 'data/processed'\n",
    "\n",
    "if not os.path.exists(processed_folder):\n",
    "    os.makedirs(processed_folder)\n",
    "\n",
    "try:\n",
    "    usage_df = pd.read_csv(usage_path)\n",
    "    factors_df = pd.read_csv(factors_path)\n",
    "    df = pd.merge(usage_df, factors_df, on='date')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    print(\"Successfully loaded and merged data.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Could not find data files in 'data/raw'. Please check the file paths.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "if not df.empty:\n",
    "    # --- 2. Create Time-Based Features ---\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "    df['day_of_month'] = df['date'].dt.day\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "\n",
    "    # --- 3. Create Lag Features (usage from the previous day) ---\n",
    "    df.sort_values(by=['region', 'resource_type', 'date'], inplace=True)\n",
    "    df['cpu_lag_1'] = df.groupby(['region', 'resource_type'])['usage_cpu'].shift(1)\n",
    "\n",
    "    # --- 4. Create Rolling Window Features (average usage over the last 7 days) ---\n",
    "    df['cpu_rolling_mean_7'] = df.groupby(['region', 'resource_type'])['usage_cpu'].transform(\n",
    "        lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    # --- 5. Display the Data with New Features ---\n",
    "    df_features = df.dropna() # Drop rows with NaN values from lag/rolling features\n",
    "\n",
    "    print(\"\\nDataset with new features (first 10 rows):\")\n",
    "    print(df_features.head(10))\n",
    "\n",
    "    # --- 6. Save the new featured dataset ---\n",
    "    featured_data_path = os.path.join(processed_folder, 'featured_dataset.csv')\n",
    "    df_features.to_csv(featured_data_path, index=False)\n",
    "    print(f\"\\nSuccessfully created new features and saved to '{featured_data_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab24f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded and merged the data files from 'data/raw'.\n",
      "\n",
      "📊 Dataset with new features (first 10 rows):\n",
      "          date   region resource_type  usage_cpu  usage_storage  users_active  \\\n",
      "14  2023-01-02  East US     Container         86           1146           220   \n",
      "26  2023-01-03  East US     Container         57           1985           414   \n",
      "38  2023-01-04  East US     Container         57           1279           232   \n",
      "50  2023-01-05  East US     Container         85           1024           359   \n",
      "62  2023-01-06  East US     Container         81           1995           432   \n",
      "74  2023-01-07  East US     Container         55           1481           439   \n",
      "86  2023-01-08  East US     Container         81           1514           208   \n",
      "98  2023-01-09  East US     Container         97           1915           334   \n",
      "110 2023-01-10  East US     Container         86            779           380   \n",
      "122 2023-01-11  East US     Container         56            535           372   \n",
      "\n",
      "     economic_index  cloud_market_demand  holiday  month  day_of_week  \\\n",
      "14           106.48                 1.15        0      1            0   \n",
      "26            97.66                 0.98        0      1            1   \n",
      "38           115.79                 1.08        0      1            2   \n",
      "50            95.31                 1.05        0      1            3   \n",
      "62            95.37                 0.95        0      1            4   \n",
      "74           102.42                 0.81        1      1            5   \n",
      "86            82.75                 0.94        1      1            6   \n",
      "98            89.87                 1.03        0      1            0   \n",
      "110           90.92                 0.86        0      1            1   \n",
      "122          114.66                 0.98        0      1            2   \n",
      "\n",
      "     day_of_month  week_of_year  cpu_lag_1  cpu_rolling_mean_7  \n",
      "14              2             1       70.0           78.000000  \n",
      "26              3             1       86.0           71.000000  \n",
      "38              4             1       57.0           67.500000  \n",
      "50              5             1       57.0           71.000000  \n",
      "62              6             1       85.0           72.666667  \n",
      "74              7             1       81.0           70.142857  \n",
      "86              8             1       55.0           71.714286  \n",
      "98              9             2       81.0           73.285714  \n",
      "110            10             2       97.0           77.428571  \n",
      "122            11             2       86.0           77.285714  \n",
      "\n",
      "✅ Successfully created new features and saved the file to: 'data/processed\\featured_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- MILESTONE 2: FEATURE ENGINEERING ---\n",
    "\n",
    "# --- 1. Define Correct File Paths ---\n",
    "# This script is now correctly set to look for your data in the 'data/raw' subfolder.\n",
    "usage_path = 'data/raw/azure_usage.csv'\n",
    "factors_path = 'data/raw/external_factors.csv.csv'\n",
    "processed_folder = 'data/processed'\n",
    "\n",
    "# Create the 'processed' folder if it doesn't already exist\n",
    "if not os.path.exists(processed_folder):\n",
    "    os.makedirs(processed_folder)\n",
    "\n",
    "# --- 2. Load and Merge Data ---\n",
    "try:\n",
    "    # Load the two CSV files from the 'data/raw' folder\n",
    "    usage_df = pd.read_csv(usage_path)\n",
    "    factors_df = pd.read_csv(factors_path)\n",
    "    \n",
    "    # Merge them on the 'date' column\n",
    "    df = pd.merge(usage_df, factors_df, on='date')\n",
    "    \n",
    "    # Convert the 'date' column to a proper datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    print(\"✅ Successfully loaded and merged the data files from 'data/raw'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ ERROR: Could not find data files in the 'data/raw' folder. Please double-check your file structure.\")\n",
    "    df = pd.DataFrame() # Create an empty DataFrame to prevent further errors\n",
    "\n",
    "# Proceed only if the data was loaded successfully\n",
    "if not df.empty:\n",
    "    # --- 3. Create Time-Based Features ---\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['day_of_month'] = df['date'].dt.day\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "\n",
    "    # --- 4. Create Lag and Rolling Features ---\n",
    "    df.sort_values(by=['region', 'resource_type', 'date'], inplace=True)\n",
    "    df['cpu_lag_1'] = df.groupby(['region', 'resource_type'])['usage_cpu'].shift(1)\n",
    "    df['cpu_rolling_mean_7'] = df.groupby(['region', 'resource_type'])['usage_cpu'].transform(\n",
    "        lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    "    )\n",
    "\n",
    "    # --- 5. Finalize and Save the New Dataset ---\n",
    "    df_features = df.dropna()\n",
    "\n",
    "    print(\"\\n📊 Dataset with new features (first 10 rows):\")\n",
    "    print(df_features.head(10))\n",
    "\n",
    "    # Save the enriched dataset to a new CSV file inside the 'data/processed' folder\n",
    "    featured_data_path = os.path.join(processed_folder, 'featured_dataset.csv')\n",
    "    df_features.to_csv(featured_data_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Successfully created new features and saved the file to: '{featured_data_path}'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda7ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 1068 records.\n",
      "Date range: 2023-01-02 00:00:00 to 2023-03-31 00:00:00\n",
      "----------------------------------------\n",
      "Data split successful!\n",
      "Training set:   747 records (2023-01-02 to 2023-03-05)\n",
      "Validation set: 214 records (2023-03-05 to 2023-03-23)\n",
      "Test set:       107 records (2023-03-23 to 2023-03-31)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your engineered dataset using the correct path and filename\n",
    "file_path = 'data/processed/featured_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# --- Pre-processing Steps ---\n",
    "# Ensure the 'date' column is a datetime object\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# Sort the dataframe by date to ensure chronological order, which is critical for time series\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded dataset with {len(df)} records.\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- Time-Based Split (70% train, 20% validation, 10% test) ---\n",
    "n = len(df)\n",
    "train_end_index = int(n * 0.7)\n",
    "validation_end_index = int(n * 0.9)\n",
    "\n",
    "# Slicing the dataframe based on index\n",
    "train_df = df.iloc[:train_end_index]\n",
    "validation_df = df.iloc[train_end_index:validation_end_index]\n",
    "test_df = df.iloc[validation_end_index:]\n",
    "\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"Data split successful!\")\n",
    "print(f\"Training set:   {len(train_df)} records ({train_df['date'].min().date()} to {train_df['date'].max().date()})\")\n",
    "print(f\"Validation set: {len(validation_df)} records ({validation_df['date'].min().date()} to {validation_df['date'].max().date()})\")\n",
    "print(f\"Test set:       {len(test_df)} records ({test_df['date'].min().date()} to {test_df['date'].max().date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c89911",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpmdarima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpm\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pmdarima\\__init__.py:52\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __check_build\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Stuff we want at top-level\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_arima, ARIMA, AutoARIMA, StepwiseContext, decompose\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m acf, autocorr_plot, c, pacf, plot_acf, plot_pacf, \\\n\u001b[32m     54\u001b[39m     tsdisplay\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pmdarima\\arima\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapprox\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marima\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pmdarima\\arima\\approx.py:9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# R approx function\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m c, check_endog\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_callable\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pmdarima\\utils\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Author: Taylor Smith <taylor.smith@alkaline-ml.com>\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetaestimators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\susha\\OneDrive\\Desktop\\AzureProject\\.venv\\Lib\\site-packages\\pmdarima\\utils\\array.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTYPE\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m C_intgrt_vec\n\u001b[32m     15\u001b[39m __all__ = [\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mas_series\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mis_iterable\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     23\u001b[39m ]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mas_series\u001b[39m(x, **kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpmdarima\\\\utils\\\\_array.pyx:1\u001b[39m, in \u001b[36minit pmdarima.utils._array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# --- NOTE: Make sure you have already run the data splitting code ---\n",
    "# We assume train_df, validation_df, and test_df are available in your notebook.\n",
    "\n",
    "# --- 1. Find the Best SARIMA Parameters and Train the Model ---\n",
    "print(\"Finding the best SARIMA model parameters... This may take a few minutes.\")\n",
    "\n",
    "# We are forecasting 'usage_cpu'.\n",
    "# We assume a weekly seasonality, so the seasonal period 'm' is 7.\n",
    "sarima_model = pm.auto_arima(train_df['usage_cpu'],\n",
    "                             start_p=1, start_q=1,\n",
    "                             test='adf',       # use adf test to find optimal 'd'\n",
    "                             max_p=3, max_q=3, # maximum p and q\n",
    "                             m=7,              # weekly seasonality\n",
    "                             d=None,           # let model determine 'd'\n",
    "                             seasonal=True,    # A SARIMA model\n",
    "                             start_P=0,\n",
    "                             D=0,\n",
    "                             trace=True,       # print status of the fits\n",
    "                             error_action='ignore',\n",
    "                             suppress_warnings=True,\n",
    "                             stepwise=True)\n",
    "\n",
    "print(\"\\n--- Best SARIMA Model Summary ---\")\n",
    "print(sarima_model.summary())\n",
    "\n",
    "# --- 2. Make Predictions on the Test Set ---\n",
    "print(\"\\nForecasting on the test set...\")\n",
    "n_periods = len(test_df)\n",
    "predictions, conf_int = sarima_model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "\n",
    "# --- 3. Evaluate the Model's Performance ---\n",
    "actuals = test_df['usage_cpu']\n",
    "\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "mape = mean_absolute_percentage_error(actuals, predictions)\n",
    "\n",
    "print(\"\\n--- SARIMA Model Evaluation Metrics ---\")\n",
    "print(f\"Mean Absolute Error (MAE):    {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
